#TER â€“ Optimization and Generalization of Neural Networks

This project corresponds to my Research Project (TER) conducted at the end of my Master 1 in IMMAEF.
It was carried out in collaboration with another student from the program, within a supervised research framework.

The objective of this work was to study the relationship between optimization methods in deep learning and the generalization properties of neural networks.
We compared several optimization algorithms and empirically analyzed the link between the geometry of minima (flat vs sharp minima) and the generalization ability of the models.

Experiments were conducted on the MNIST dataset, achieving an accuracy of 98.7%.
The results allowed us to empirically validate the chain of indicators linking PAC-Bayes theory, training stability, and generalization on the reference runs.
