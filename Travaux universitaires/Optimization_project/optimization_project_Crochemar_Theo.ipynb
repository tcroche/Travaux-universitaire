{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question 2",
   "id": "b891ee7d6ade3ad4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:05.048520Z",
     "start_time": "2026-02-11T17:59:04.449880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ],
   "id": "5ccd2dc1ed2b19f9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:06.916541Z",
     "start_time": "2026-02-11T17:59:06.908615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_mu_sigma():\n",
    "    \"\"\"We build mu and Sigma.\"\"\"\n",
    "    mu = np.array([\n",
    "        3.66, 1.33, 2.51, 2.24, 2.62, 2.03,\n",
    "        1.91, 1.39, 2.61, 3.58, 5.23, 8.36\n",
    "    ], dtype=float)\n",
    "\n",
    "    Sigma = np.array([\n",
    "        [ 73.73,  40.79,  39.46,  38.01,  49.40,  58.51,  46.08,  42.26,  -1.22,  12.44,  -2.31, 131.17],\n",
    "        [ 40.79,  39.54,  32.15,  34.43,  34.25,  36.88,  33.11,  26.50,  -3.13,  19.06,   0.53, 136.78],\n",
    "        [ 39.46,  32.15,  54.98,  31.62,  33.80,  41.20,  26.48,  29.76,  -2.90,  25.34,  -3.62, 151.82],\n",
    "        [ 38.01,  34.43,  31.62,  39.61,  35.73,  33.64,  38.12,  26.00,  -0.08,  13.23,  -5.61,  73.96],\n",
    "        [ 49.40,  34.25,  33.80,  35.73,  68.10,  36.88,  37.02,  31.64,  14.42,   3.33,   1.43,  -0.12],\n",
    "        [ 58.51,  36.88,  41.20,  33.64,  36.88,  66.93,  40.89,  39.08, -13.71,  22.07, -14.96, 210.26],\n",
    "        [ 46.08,  33.11,  26.48,  38.12,  37.02,  40.89,  75.40,  38.58,  19.01,  19.37,  30.95,  52.63],\n",
    "        [ 42.26,  26.50,  29.76,  26.00,  31.64,  39.08,  38.58,  47.35,  -3.53,  17.02,  -8.28, 111.37],\n",
    "        [ -1.22,  -3.13,  -2.90,  -0.08,  14.42, -13.71,  19.01,  -3.53, 242.03, -90.65, 196.18,-507.15],\n",
    "        [ 12.44,  19.06,  25.34,  13.23,   3.33,  22.07,  19.37,  17.02, -90.65, 110.87, -50.97, 360.36],\n",
    "        [ -2.31,   0.53,  -3.62,  -5.61,   1.43, -14.96,  30.95,  -8.28, 196.18, -50.97, 428.23,-504.38],\n",
    "        [131.17, 136.78, 151.82,  73.96,  -0.12, 210.26,  52.63, 111.37,-507.15, 360.36,-504.38,2863.07]\n",
    "    ], dtype=float)\n",
    "\n",
    "    return mu, Sigma\n",
    "\n",
    "\n",
    "def build_reduced_quadratic(mu, Sigma, phi=5.0):\n",
    "    \"\"\"\n",
    "    We build Q, q, r such that Je(x) = x^T Q x + q^T x + r,\n",
    "    where x in R^(n-1) and x_n = 1 - sum_{i=1}^{n-1} x_i.\n",
    "    \"\"\"\n",
    "    n = mu.shape[0]\n",
    "    m = n - 1\n",
    "    e = np.ones(m)\n",
    "\n",
    "    A = Sigma[:m, :m]\n",
    "    b = Sigma[:m, m]\n",
    "    c = Sigma[m, m]\n",
    "\n",
    "    Q = A - np.outer(b, e) - np.outer(e, b) + c * np.outer(e, e)\n",
    "    q = 2.0 * b - 2.0 * c * e - phi * (mu[:m] - mu[m] * e)\n",
    "    r = c - phi * mu[m]\n",
    "\n",
    "    return Q, q, r\n"
   ],
   "id": "debaf8debd1c19cb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:07.659951Z",
     "start_time": "2026-02-11T17:59:07.654504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def Je(x, Q, q, r):\n",
    "    \"\"\"It is the reduced solution Je(x) = x^T Q x + q^T x + r.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return float(x @ Q @ x + q @ x + r)\n",
    "\n",
    "\n",
    "def grad_Je(x, Q, q):\n",
    "    \"\"\"It is the gradient of Je: ∇Je(x) = 2Qx + q.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return 2.0 * (Q @ x) + q"
   ],
   "id": "4b8c13cc0b2f4919",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:08.318557Z",
     "start_time": "2026-02-11T17:59:08.311495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def line_search_strong_wolfe(f, grad, xk, pk, c1=1e-4, c2=0.9, alpha1=1.0, alpha_max=1e6, max_iter=50):\n",
    "    \"\"\"\n",
    "    Strong Wolfe line search for phi(alpha)=f(xk+alpha pk).\n",
    "    It uses a bisection-based zoom procedure.\n",
    "    \"\"\"\n",
    "    def phi(a):\n",
    "        return f(xk + a * pk)\n",
    "\n",
    "    def dphi(a):\n",
    "        return float(grad(xk + a * pk) @ pk)\n",
    "\n",
    "    phi0 = phi(0.0)\n",
    "    dphi0 = dphi(0.0)\n",
    "    if dphi0 >= 0.0:\n",
    "        # pk is not a descent direction\n",
    "        return 0.0, False\n",
    "\n",
    "    alpha_prev = 0.0\n",
    "    phi_prev = phi0\n",
    "    alpha = alpha1\n",
    "\n",
    "    def zoom(alow, ahigh, phi_low):\n",
    "        for _ in range(60):\n",
    "            a = 0.5 * (alow + ahigh)  # bisection\n",
    "            phi_a = phi(a)\n",
    "\n",
    "            if (phi_a > phi0 + c1 * a * dphi0) or (phi_a >= phi_low):\n",
    "                ahigh = a\n",
    "            else:\n",
    "                dphi_a = dphi(a)\n",
    "                if abs(dphi_a) <= -c2 * dphi0:\n",
    "                    return a, True\n",
    "                if dphi_a * (ahigh - alow) >= 0:\n",
    "                    ahigh = alow\n",
    "                alow = a\n",
    "                phi_low = phi_a\n",
    "\n",
    "            if abs(ahigh - alow) < 1e-14:\n",
    "                return a, False\n",
    "\n",
    "        return a, False\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        phi_a = phi(alpha)\n",
    "\n",
    "        if (phi_a > phi0 + c1 * alpha * dphi0) or (i > 0 and phi_a >= phi_prev):\n",
    "            return zoom(alpha_prev, alpha, phi_prev)\n",
    "\n",
    "        dphi_a = dphi(alpha)\n",
    "        if abs(dphi_a) <= -c2 * dphi0:\n",
    "            return alpha, True\n",
    "\n",
    "        if dphi_a >= 0:\n",
    "            return zoom(alpha, alpha_prev, phi_a)\n",
    "\n",
    "        alpha_prev = alpha\n",
    "        phi_prev = phi_a\n",
    "        alpha = min(2.0 * alpha, alpha_max)\n",
    "\n",
    "    return alpha, False"
   ],
   "id": "4ddcc7470fb1b62d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:08.973593Z",
     "start_time": "2026-02-11T17:59:08.967659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def steepest_descent_wolfe(Q, q, r, x0, tol=1e-5, max_iter=200000, c1=1e-4, c2=0.9):\n",
    "    \"\"\"\n",
    "    Steepest descent:\n",
    "        p_k = -∇Je(x_k)\n",
    "        x_{k+1} = x_k + alpha_k p_k\n",
    "    with alpha_k satisfying the strong Wolfe conditions.\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        gk = grad_Je(x, Q, q)\n",
    "        if np.linalg.norm(gk) <= tol:\n",
    "            return x, k, Je(x, Q, q, r), True\n",
    "\n",
    "        pk = -gk\n",
    "\n",
    "        f = lambda z: Je(z, Q, q, r)\n",
    "        g = lambda z: grad_Je(z, Q, q)\n",
    "\n",
    "        alpha, ok = line_search_strong_wolfe(f, g, x, pk, c1=c1, c2=c2)\n",
    "\n",
    "        # We perform a fallback which is the exact step for a quadratic to ensure progress\n",
    "        if not ok:\n",
    "            denom = pk @ (2.0 * (Q @ pk))\n",
    "            alpha = (gk @ gk) / denom\n",
    "\n",
    "        x = x + alpha * pk\n",
    "\n",
    "    return x, max_iter, Je(x, Q, q, r), False"
   ],
   "id": "d84854c2c73f0034",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:15.285297Z",
     "start_time": "2026-02-11T17:59:09.620874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "phi = 5.0\n",
    "mu, Sigma = build_mu_sigma()\n",
    "Q, q, r = build_reduced_quadratic(mu, Sigma, phi=phi)\n",
    "\n",
    "# Starting point, we'll keep the same starting point throughout the project\n",
    "x0 = np.zeros(11)\n",
    "\n",
    "x_star, n_iter, Je_star, converged = steepest_descent_wolfe(Q, q, r, x0, tol=1e-5)\n",
    "\n",
    "print(\"Converged:\", converged)\n",
    "print(\"Starting point x0 =\", x0)\n",
    "print(\"Iterations =\", n_iter)\n",
    "print(\"Optimal reduced x* =\", x_star)\n",
    "print(\"Je(x*) =\", Je_star)"
   ],
   "id": "b160e4790cd528f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged: True\n",
      "Starting point x0 = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iterations = 55150\n",
      "Optimal reduced x* = [ 0.2685102  -0.6015868  -0.18651315  0.95014298  0.05797521 -0.01420054\n",
      " -0.39572757  0.18019928  0.24183311  0.41863451  0.06426102]\n",
      "Je(x*) = 3.0412849729891605\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 3\n",
    "We'll reuse what we've already coded in 2 question and add the definition for the Hessian matrix of Je, we'll code when wolfe strong is satisfied, redefine the line search strong wolfe and we'll define the Newton-Wolfe method"
   ],
   "id": "6bfbc164bbeb60cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:17.300490Z",
     "start_time": "2026-02-11T17:59:17.296286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hess_Je(Q):\n",
    "    \"\"\"The hessian matrix of Je: ∇^2Je(x) = 2Q.\"\"\"\n",
    "    return 2.0 * Q"
   ],
   "id": "80cd6c85e2293963",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:18.381418Z",
     "start_time": "2026-02-11T17:59:18.376416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def wolfe_strong_satisfied(f, grad, xk, pk, alpha, c1=1e-4, c2=0.9):\n",
    "    \"\"\"\n",
    "    Check strong Wolfe conditions for phi(alpha)=f(xk+alpha pk).\n",
    "    Returns True if both Armijo and strong curvature are satisfied.\n",
    "    \"\"\"\n",
    "    phi0 = f(xk)\n",
    "    g0 = grad(xk)\n",
    "    dphi0 = float(g0 @ pk)\n",
    "\n",
    "    # pk must be a descent direction for Wolfe\n",
    "    if dphi0 >= 0.0:\n",
    "        return False\n",
    "\n",
    "    x_new = xk + alpha * pk\n",
    "    phi_a = f(x_new)\n",
    "    ga = grad(x_new)\n",
    "    dphi_a = float(ga @ pk)\n",
    "\n",
    "    armijo = (phi_a <= phi0 + c1 * alpha * dphi0)\n",
    "    curvature = (abs(dphi_a) <= -c2 * dphi0)\n",
    "    return armijo and curvature"
   ],
   "id": "7b98a6a7296c5db9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:19.167928Z",
     "start_time": "2026-02-11T17:59:19.160950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def line_search_strong_wolfe(f, grad, xk, pk, c1=1e-4, c2=0.9, alpha1=1.0, alpha_max=1e6, max_iter=50):\n",
    "    \"\"\"\n",
    "    Strong Wolfe line search.\n",
    "    Uses bracketing then a bisection zoom.\n",
    "    \"\"\"\n",
    "    phi0 = f(xk)\n",
    "    g0 = grad(xk)\n",
    "    dphi0 = float(g0 @ pk)\n",
    "    if dphi0 >= 0.0:\n",
    "        return 0.0, False\n",
    "\n",
    "    def phi(a):\n",
    "        return f(xk + a * pk)\n",
    "\n",
    "    def dphi(a):\n",
    "        return float(grad(xk + a * pk) @ pk)\n",
    "\n",
    "    alpha_prev = 0.0\n",
    "    phi_prev = phi0\n",
    "    alpha = alpha1\n",
    "\n",
    "    # Bracketing phase\n",
    "    for i in range(max_iter):\n",
    "        phi_a = phi(alpha)\n",
    "\n",
    "        if (phi_a > phi0 + c1 * alpha * dphi0) or (i > 0 and phi_a >= phi_prev):\n",
    "            a_lo, a_hi = alpha_prev, alpha\n",
    "            break\n",
    "\n",
    "        dphi_a = dphi(alpha)\n",
    "        if abs(dphi_a) <= -c2 * dphi0:\n",
    "            return alpha, True\n",
    "\n",
    "        if dphi_a >= 0.0:\n",
    "            a_lo, a_hi = alpha, alpha_prev\n",
    "            break\n",
    "\n",
    "        alpha_prev = alpha\n",
    "        phi_prev = phi_a\n",
    "        alpha = min(2.0 * alpha, alpha_max)\n",
    "    else:\n",
    "        return alpha, False\n",
    "\n",
    "    # Ensure ordering for zoom\n",
    "    a_lo, a_hi = (min(a_lo, a_hi), max(a_lo, a_hi))\n",
    "    phi_lo = phi(a_lo)\n",
    "\n",
    "    # Zoom phase\n",
    "    for _ in range(60):\n",
    "        a = 0.5 * (a_lo + a_hi)\n",
    "        phi_a = phi(a)\n",
    "\n",
    "        if (phi_a > phi0 + c1 * a * dphi0) or (phi_a >= phi_lo):\n",
    "            a_hi = a\n",
    "        else:\n",
    "            dphi_a = dphi(a)\n",
    "            if abs(dphi_a) <= -c2 * dphi0:\n",
    "                return a, True\n",
    "            if dphi_a * (a_hi - a_lo) >= 0.0:\n",
    "                a_hi = a_lo\n",
    "            a_lo = a\n",
    "            phi_lo = phi_a\n",
    "\n",
    "        if abs(a_hi - a_lo) < 1e-14:\n",
    "            return a, False\n",
    "\n",
    "    return a, False"
   ],
   "id": "260ef1bac11371fa",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:19.760546Z",
     "start_time": "2026-02-11T17:59:19.754426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def newton_wolfe(Q, q, r, x0, tol=1e-8, max_iter=50, c1=1e-4, c2=0.9):\n",
    "    \"\"\"\n",
    "    Newton's method with strong Wolfe step lengths.\n",
    "\n",
    "    Key robustness features:\n",
    "    - Try alpha=1 first and accept it if it satisfies strong Wolfe.\n",
    "    - Check convergence both before and after the update.\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "\n",
    "    # Symmetrize Q to reduce numerical asymmetry (good practice)\n",
    "    Qs = 0.5 * (Q + Q.T)\n",
    "    H = 2.0 * Qs  # constant Hessian\n",
    "\n",
    "    f = lambda z: Je(z, Qs, q, r)\n",
    "    g = lambda z: grad_Je(z, Qs, q)\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        grad_k = g(x)\n",
    "        if np.linalg.norm(grad_k) <= tol:\n",
    "            return x, k, f(x), True\n",
    "\n",
    "        # Newton direction: solve H p = -grad\n",
    "        pk = np.linalg.solve(H, -grad_k)\n",
    "\n",
    "        # We first try full Newton step\n",
    "        alpha = 1.0\n",
    "        if not wolfe_strong_satisfied(f, g, x, pk, alpha, c1=c1, c2=c2):\n",
    "            # Otherwise we run Wolfe line search\n",
    "            alpha, ok = line_search_strong_wolfe(f, g, x, pk, c1=c1, c2=c2, alpha1=1.0)\n",
    "            if not ok:\n",
    "                # Safe fallback\n",
    "                alpha = 1.0\n",
    "\n",
    "        x = x + alpha * pk\n",
    "\n",
    "        # Check convergence right after the update\n",
    "        if np.linalg.norm(g(x)) <= tol:\n",
    "            return x, k + 1, f(x), True\n",
    "\n",
    "    return x, max_iter, f(x), False"
   ],
   "id": "fd3bcaae59227097",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:20.747453Z",
     "start_time": "2026-02-11T17:59:20.735926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "phi = 5.0\n",
    "mu, Sigma = build_mu_sigma()\n",
    "Q, q, r = build_reduced_quadratic(mu, Sigma, phi=phi)\n",
    "\n",
    "# We take the same starting point as in Q2\n",
    "x0 = np.zeros(11)\n",
    "\n",
    "x_star, n_iter, Je_star, converged = newton_wolfe(Q, q, r, x0)\n",
    "\n",
    "print(\"Converged:\", converged)\n",
    "print(\"Starting point x0 =\", x0)\n",
    "print(\"Iterations =\", n_iter)\n",
    "print(\"Optimal reduced x* =\", x_star)\n",
    "print(\"Je(x*) =\", Je_star)\n",
    "3.0412849729891605"
   ],
   "id": "74b7489e85f6dc9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged: True\n",
      "Starting point x0 = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iterations = 1\n",
      "Optimal reduced x* = [ 0.26851035 -0.60158785 -0.18651334  0.95014403  0.05797526 -0.01420058\n",
      " -0.39572781  0.18019933  0.24183317  0.41863455  0.06426108]\n",
      "Je(x*) = 3.041284972984613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0412849729891605"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question 4\n",
   "id": "7e0c822a6d6c0847"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:22.376826Z",
     "start_time": "2026-02-11T17:59:22.371322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def conjugate_gradient(A, b, x0=None, tol=1e-10, max_iter=None):\n",
    "    \"\"\"\n",
    "    Linear Conjugate Gradient to solve A x = b for SPD matrix A.\n",
    "    Stops when ||r_k|| <= tol where r_k = b - A x_k.\n",
    "    \"\"\"\n",
    "    n = b.size\n",
    "    x = np.zeros(n, dtype=float) if x0 is None else np.array(x0, dtype=float)\n",
    "\n",
    "    r = b - A @ x\n",
    "    p = r.copy()\n",
    "    rs_old = float(r @ r)\n",
    "\n",
    "    if max_iter is None:\n",
    "        max_iter = n  # theoretical upper bound\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        Ap = A @ p\n",
    "        alpha = rs_old / float(p @ Ap)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "\n",
    "        rs_new = float(r @ r)\n",
    "        if np.sqrt(rs_new) <= tol:\n",
    "            return x, k + 1, np.sqrt(rs_new), True\n",
    "\n",
    "        beta = rs_new / rs_old\n",
    "        p = r + beta * p\n",
    "        rs_old = rs_new\n",
    "\n",
    "    return x, max_iter, np.sqrt(rs_old), False"
   ],
   "id": "b2d60eef567ec1a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:23.381613Z",
     "start_time": "2026-02-11T17:59:23.375765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "phi = 5.0\n",
    "mu, Sigma = build_mu_sigma()\n",
    "Q, q, r = build_reduced_quadratic(mu, Sigma, phi=phi)\n",
    "\n",
    "# Minimizer solves (2Q) x = -q\n",
    "H = 2.0 * Q\n",
    "b = -q\n",
    "\n",
    "x0 = np.zeros(11)\n",
    "x_star, n_iter, res_norm, converged = conjugate_gradient(H, b, x0=x0, tol=1e-10, max_iter=200)\n",
    "\n",
    "print(\"Converged:\", converged)\n",
    "print(\"Iterations:\", n_iter)\n",
    "print(\"Residual norm ||b - Hx||:\", res_norm)\n",
    "print(\"x* (reduced):\", x_star)\n",
    "print(\"Je(x*):\", Je(x_star, Q, q, r))\n"
   ],
   "id": "1953d4d126ef7e83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged: True\n",
      "Iterations: 16\n",
      "Residual norm ||b - Hx||: 8.574691791826074e-11\n",
      "x* (reduced): [ 0.26851035 -0.60158785 -0.18651334  0.95014403  0.05797526 -0.01420058\n",
      " -0.39572781  0.18019933  0.24183317  0.41863455  0.06426108]\n",
      "Je(x*): 3.0412849729832487\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Question 5",
   "id": "c58d3f29aa410ecd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:30.482237Z",
     "start_time": "2026-02-11T17:59:30.474188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def solve_with_scipy(Q, q, r, x0):\n",
    "    \"\"\"\n",
    "    Solve min Je(x) using scipy.optimize.minimize with BFGS.\n",
    "    We provide the analytic gradient to speed up convergence and improve accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    fun = lambda x: Je(x, Q, q, r)\n",
    "    jac = lambda x: grad_Je(x, Q, q)\n",
    "\n",
    "    # We'll use Scipy since it has BFGS implemented in it and it's a quasi-Newton method\n",
    "    res = minimize(\n",
    "        fun=fun,\n",
    "        x0=x0,\n",
    "        jac=jac,\n",
    "        method=\"BFGS\",\n",
    "        options={\n",
    "            \"gtol\": 1e-10,      # stop when ||grad|| is small\n",
    "            \"maxiter\": 10_000,\n",
    "            \"disp\": False\n",
    "        }\n",
    "    )\n",
    "    return res\n",
    "def newton_exact_quadratic(Q, q):\n",
    "    \"\"\"\n",
    "    For Je(x)=x^T Q x + q^T x + r, the minimizer satisfies 2Qx + q = 0,\n",
    "    hence x* = -0.5 Q^{-1} q (assuming Q is invertible / SPD in practice).\n",
    "    \"\"\"\n",
    "    return np.linalg.solve(2.0 * Q, -q)"
   ],
   "id": "858d0d22fc692374",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T17:59:31.500660Z",
     "start_time": "2026-02-11T17:59:31.488571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "phi = 5.0\n",
    "mu, Sigma = build_mu_sigma()\n",
    "Q, q, r = build_reduced_quadratic(mu, Sigma, phi=phi)\n",
    "\n",
    "# We use the same starting point as in previous questions\n",
    "x0 = np.zeros(11)\n",
    "\n",
    "# The solution with scipy\n",
    "res = solve_with_scipy(Q, q, r, x0)\n",
    "x_scipy = res.x\n",
    "Je_scipy = Je(x_scipy, Q, q, r)\n",
    "grad_norm_scipy = np.linalg.norm(grad_Je(x_scipy, Q, q))\n",
    "\n",
    "# The solution we got, as a comparison, from question 3\n",
    "x_newton, it_newton, Je_newton, ok_newton = newton_wolfe(Q, q, r, x0, tol=1e-10)\n",
    "grad_norm_newton = np.linalg.norm(grad_Je(x_newton, Q, q))\n",
    "\n",
    "# We'll use these metrics to compare BFGS and the one we used in question 3\n",
    "diff_x = np.linalg.norm(x_scipy - x_newton)\n",
    "diff_J = abs(Je_scipy - Je_newton)\n",
    "\n",
    "print(\"---- SciPy (BFGS) ----\")\n",
    "print(\"success:\", res.success)\n",
    "print(\"iterations (nit):\", res.nit, \"| nfev:\", res.nfev, \"| njev:\", res.njev)\n",
    "print(\"Je(x*) =\", Je_scipy)\n",
    "print(\"||grad|| =\", grad_norm_scipy)\n",
    "\n",
    "print(\"\\n---- Our Newton+Wolfe (Q3) ----\")\n",
    "print(\"converged:\", ok_newton)\n",
    "print(\"iterations:\", it_newton)\n",
    "print(\"Je(x*) =\", Je_newton)\n",
    "print(\"||grad|| =\", grad_norm_newton)\n",
    "\n",
    "print(\"\\n---- Comparison ----\")\n",
    "print(\"||x_scipy - x_newton|| =\", diff_x)\n",
    "print(\"|Je_scipy - Je_newton| =\", diff_J)"
   ],
   "id": "51397345128ed0be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- SciPy (BFGS) ----\n",
      "success: True\n",
      "iterations (nit): 16 | nfev: 21 | njev: 21\n",
      "Je(x*) = 3.0412849729841582\n",
      "||grad|| = 5.903314256149596e-11\n",
      "\n",
      "---- Our Newton+Wolfe (Q3) ----\n",
      "converged: True\n",
      "iterations: 1\n",
      "Je(x*) = 3.041284972984613\n",
      "||grad|| = 2.227797943108433e-12\n",
      "\n",
      "---- Comparison ----\n",
      "||x_scipy - x_newton|| = 6.390720527027872e-13\n",
      "|Je_scipy - Je_newton| = 4.547473508864641e-13\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Question 6\n",
    "Now we once again consider the constrains in order to solve the optimization problem. We'll go back to the full 12-weight vector for simpler computations."
   ],
   "id": "bd1ad3f1dd493eb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T18:12:38.996129Z",
     "start_time": "2026-02-11T18:12:38.990781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective_full(w, Sigma, mu, phi=5.0):\n",
    "    \"\"\"Objective J(w) = w^T Sigma w - phi * mu^T w (full 12D variable on the simplex).\"\"\"\n",
    "    return float(w @ Sigma @ w - phi * (mu @ w))\n",
    "\n",
    "\n",
    "def grad_full(w, Sigma, mu, phi=5.0):\n",
    "    \"\"\"Gradient of J: ∇J(w) = 2 Sigma w - phi mu.\"\"\"\n",
    "    return 2.0 * (Sigma @ w) - phi * mu\n",
    "\n",
    "def project_to_simplex(v):\n",
    "    \"\"\"\n",
    "    Euclidean projection onto the simplex {w: w >= 0, sum(w) = 1}.\n",
    "    Algorithm: sort v, find threshold theta, then w = max(v - theta, 0).\n",
    "    \"\"\"\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    n = v.size\n",
    "\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.where(u - (cssv - 1.0) / (np.arange(n) + 1) > 0)[0][-1]\n",
    "    theta = (cssv[rho] - 1.0) / (rho + 1)\n",
    "\n",
    "    w = np.maximum(v - theta, 0.0)\n",
    "    return w"
   ],
   "id": "65f3de74c3ccdd64",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T18:12:41.846910Z",
     "start_time": "2026-02-11T18:12:41.840665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def projected_gradient_descent_simplex(Sigma, mu, phi=5.0, w0=None, tol=1e-8, max_iter=200000):\n",
    "    \"\"\"\n",
    "    Projected Gradient Descent on the simplex with Armijo backtracking.\n",
    "    We backtrack alpha until Armijo decrease holds along the feasible direction d.\n",
    "    \"\"\"\n",
    "    n = mu.size\n",
    "    if w0 is None:\n",
    "        w = np.ones(n) / n\n",
    "    else:\n",
    "        w = project_to_simplex(w0)\n",
    "\n",
    "    f = lambda x: objective_full(x, Sigma, mu, phi)\n",
    "    g = lambda x: grad_full(x, Sigma, mu, phi)\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        grad = g(w)\n",
    "        # Projected gradient mapping\n",
    "        w_proj = project_to_simplex(w - grad)\n",
    "        pg_norm = np.linalg.norm(w - w_proj)\n",
    "\n",
    "        if pg_norm <= tol:\n",
    "            return w, k, f(w), True\n",
    "\n",
    "        # We perform a Armijo backtracking on the projected step\n",
    "        alpha = 1.0\n",
    "        c1 = 1e-4\n",
    "        f_w = f(w)\n",
    "\n",
    "        while True:\n",
    "            w_new = project_to_simplex(w - alpha * grad)\n",
    "            d = w_new - w\n",
    "\n",
    "            # If the projection gives almost no movement, we consider that we are numerically stationary\n",
    "            if np.linalg.norm(d) < 1e-14:\n",
    "                return w, k, f_w, True\n",
    "\n",
    "            # Armijo sufficient decrease using directional derivative of the grad^T d\n",
    "            if f(w_new) <= f_w + c1 * (grad @ d):\n",
    "                break\n",
    "\n",
    "            alpha *= 0.5\n",
    "            if alpha < 1e-16:\n",
    "                # If the step became too small, we stop safely\n",
    "                return w, k, f_w, False\n",
    "\n",
    "        w = w_new\n",
    "\n",
    "    return w, max_iter, f(w), False"
   ],
   "id": "9d68a3d726d83cd9",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T18:12:44.058505Z",
     "start_time": "2026-02-11T18:12:43.977403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "phi = 5.0\n",
    "mu, Sigma = build_mu_sigma()\n",
    "\n",
    "w_star, n_iter, J_star, ok = projected_gradient_descent_simplex(\n",
    "    Sigma=Sigma,\n",
    "    mu=mu,\n",
    "    phi=phi,\n",
    "    w0=np.ones(12) / 12,\n",
    "    tol=1e-8,\n",
    "    max_iter=200000\n",
    ")\n",
    "\n",
    "print(\"Converged:\", ok)\n",
    "print(\"Iterations:\", n_iter)\n",
    "print(\"w* =\", w_star)\n",
    "print(\"sum(w*) =\", w_star.sum(), \"min(w*) =\", w_star.min())\n",
    "print(\"J(w*) =\", J_star)\n",
    "\n",
    "# Reduced variable x in R^11\n",
    "x_star = w_star[:11]\n",
    "print(\"x* (reduced) =\", x_star)\n",
    "print(\"constraint 1 - sum(x*) =\", 1.0 - x_star.sum())"
   ],
   "id": "db7e23ec9144d667",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged: True\n",
      "Iterations: 275\n",
      "w* = [0.09546618 0.         0.         0.27193496 0.03241073 0.\n",
      " 0.         0.03515613 0.20256739 0.34348564 0.01897897 0.        ]\n",
      "sum(w*) = 0.9999999999999987 min(w*) = 0.0\n",
      "J(w*) = 7.918837100790903\n",
      "x* (reduced) = [0.09546618 0.         0.         0.27193496 0.03241073 0.\n",
      " 0.         0.03515613 0.20256739 0.34348564 0.01897897]\n",
      "constraint 1 - sum(x*) = 1.3322676295501878e-15\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1cd68913374b5680"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
